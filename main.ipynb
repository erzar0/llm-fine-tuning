{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:42:11.700537800Z",
     "start_time": "2025-05-03T16:42:11.618205200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud2020/0starzyk/llm-fine-tuning/my-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = \".\"\n",
    "print(NOTEBOOK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:42:27.943605900Z",
     "start_time": "2025-05-03T16:42:11.644206700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data subsets: dict_keys(['train', 'test'])\n",
      "Features: \n",
      "---> id                            : 5097\n",
      "---> domain                        : forestry\n",
      "---> domain_description            : Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.\n",
      "---> sql_complexity                : single join\n",
      "---> sql_complexity_description    : only one join (specify inner, outer, cross)\n",
      "---> sql_task_type                 : analytics and reporting\n",
      "---> sql_task_type_description     : generating reports, dashboards, and analytical insights\n",
      "---> sql_prompt                    : What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "---> sql_context                   : CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "---> sql                           : SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "---> sql_explanation               : Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/gretelai/synthetic_text_to_sql\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "\n",
    "print(\"Available data subsets:\", dataset.keys())\n",
    "print(\"Features: \")\n",
    "for k, v in dataset[\"train\"][0].items():\n",
    "    print(f\"---> {k:30}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:43:15.028542700Z",
     "start_time": "2025-05-03T16:42:27.948187200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation']\n",
      "Checking 'train' split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████▌                                                     | 1/2 [00:24<00:24, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sql_prompt: 0 missing or empty entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:49<00:00, 24.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sql: 0 missing or empty entries\n",
      "\n",
      "Checking 'test' split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sql_prompt: 0 missing or empty entries\n",
      "  sql: 0 missing or empty entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def check_missing_data(dataset, columns):\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Checking '{split}' split...\")\n",
    "        for column in tqdm(columns):\n",
    "            if column not in dataset[split].column_names:\n",
    "                print(f\"  Column '{column}' not found in the dataset!\")\n",
    "                continue\n",
    "            missing_count = sum(1 for example in dataset[split] if not example[column] or example[column].strip() == \"\")\n",
    "            print(f\"  {column}: {missing_count} missing or empty entries\")\n",
    "        print()\n",
    "\n",
    "print(\"Dataset columns:\", dataset[\"train\"].column_names)\n",
    "check_missing_data(dataset, columns=[\"sql_prompt\", \"sql\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved ShareGPT formatted data to 'data/dataset_train.json'\n",
      "\n",
      "--- First Record Example ---\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"user\",\n",
      "      \"value\": \"Context:\\n'CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');'\\n                                    Question:\\n'What is the total volume of timber sold by each salesperson, sorted by salesperson?'\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"Result: 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;'\"\n",
      "    }\n",
      "  ],\n",
      "  \"system\": \"You are a helpful assistant specialized in generating SQL queries.\\nGenerate an SQL query that correctly answers the user's question based on the provided database schema and context.\",\n",
      "  \"tools\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import create_sharegpt_format\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "sharegpt_data = create_sharegpt_format(train_df)\n",
    "\n",
    "output_filename = Path('data/dataset_train.json')\n",
    "try:\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sharegpt_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Successfully saved ShareGPT formatted data to '{output_filename}'\")\n",
    "\n",
    "    if sharegpt_data:\n",
    "        print(\"\\n--- First Record Example ---\")\n",
    "        print(json.dumps(sharegpt_data[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")\n",
    "\n",
    "!mkdir -p $NOTEBOOK_DIR/LLaMA-Factory/data/\n",
    "!cp -r $NOTEBOOK_DIR/data/ $NOTEBOOK_DIR/LLaMA-Factory/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202\n"
     ]
    }
   ],
   "source": [
    "max_len_str = 0\n",
    "for record in sharegpt_data:\n",
    "    max_len_str = max(max_len_str, len(record['conversations'][0]['value']))\n",
    "print(max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 357, done.\u001b[K\n",
      "remote: Counting objects: 100% (357/357), done.\u001b[K\n",
      "remote: Compressing objects: 100% (276/276), done.\u001b[K\n",
      "remote: Total 357 (delta 77), reused 292 (delta 66), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (357/357), 9.64 MiB | 12.66 MiB/s, done.\n",
      "Resolving deltas: 100% (77/77), done.\n",
      "Updating files: 100% (290/290), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stud2020/0starzyk/llm-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "%cd /home/stud2020/0starzyk/llm-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stud2020/0starzyk/llm-fine-tuning/LLaMA-Factory\n",
      "\u001b[0m\u001b[01;34mdata\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd $NOTEBOOK_DIR/LLaMA-Factory\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://50c68308057112391c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    }
   ],
   "source": [
    "# !GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    # \"deepspeed\": DS_CONFIG_PATH,\n",
    "    \"cutoff_len\": 1024,\n",
    "    \"dataset\": \"train_sql_dataset\",\n",
    "    \"ddp_timeout\": 9000,\n",
    "    \"do_train\": True,\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    \"use_dora\": True,\n",
    "    \"fp16\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"flash_attn\": \"fa2\",\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"logging_steps\": 8,\n",
    "    \"lora_target\": \"q_proj,v_proj\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"output_dir\": \"out\",\n",
    "    \"overwrite_cache\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"plot_loss\": True,\n",
    "    \"report_to\": None, # wnadb\n",
    "    \"save_steps\": 1000,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"stage\": \"sft\",\n",
    "    \"template\": \"qwen3\",\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "\n",
    "json.dump(training_args, open(f\"{NOTEBOOK_DIR}/LLaMA-Factory/train_qwen3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-05-11 23:08:29] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cpu, distributed training: False, compute dtype: torch.float16\n",
      "tokenizer_config.json: 100%|███████████████| 9.68k/9.68k [00:00<00:00, 16.9MB/s]\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 4.98MB/s]\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 3.72MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:01<00:00, 8.06MB/s]\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:34,583 >> loading file vocab.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:34,584 >> loading file merges.txt from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:34,584 >> loading file tokenizer.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:34,584 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:34,584 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:34,584 >> loading file tokenizer_config.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:34,584 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-11 23:08:35,519 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100%|█████████████████████████████| 726/726 [00:00<00:00, 3.28MB/s]\n",
      "[INFO|configuration_utils.py:693] 2025-05-11 23:08:36,385 >> loading configuration file config.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 23:08:36,401 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:36,572 >> loading file vocab.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:36,573 >> loading file merges.txt from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:36,573 >> loading file tokenizer.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:36,573 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:36,573 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:36,573 >> loading file tokenizer_config.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 23:08:36,573 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-11 23:08:37,241 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-11 23:08:37] llamafactory.data.loader:143 >> Loading dataset dataset_train.json...\n",
      "Generating train split: 100000 examples [00:03, 32317.77 examples/s]\n",
      "Converting format of dataset: 100%|█| 100000/100000 [00:27<00:00, 3612.52 exampl\n",
      "Running tokenizer on dataset: 100%|█| 100000/100000 [02:21<00:00, 706.08 example\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 27076, 304, 23163, 7870, 19556, 624, 31115, 458, 7870, 3239, 429, 12440, 11253, 279, 1196, 594, 3405, 3118, 389, 279, 3897, 4625, 10802, 323, 2266, 13, 151645, 198, 151644, 872, 198, 1972, 510, 6, 22599, 14363, 6625, 8987, 320, 29041, 8987, 842, 9221, 11, 829, 15762, 11, 5537, 15762, 1215, 39518, 12496, 6625, 8987, 320, 29041, 8987, 842, 11, 829, 11, 5537, 8, 14710, 320, 16, 11, 364, 13079, 49628, 516, 364, 25221, 4567, 320, 17, 11, 364, 62502, 9082, 516, 364, 25018, 4667, 30776, 14363, 44788, 47067, 320, 29041, 842, 9221, 11, 6625, 8987, 842, 9221, 11, 8123, 25272, 11, 6278, 4164, 28543, 1215, 39518, 12496, 44788, 47067, 320, 29041, 842, 11, 6625, 8987, 842, 11, 8123, 11, 6278, 4164, 8, 14710, 320, 16, 11, 220, 16, 11, 220, 16, 17, 15, 11, 364, 17, 15, 17, 16, 12, 15, 16, 12, 15, 16, 4567, 320, 17, 11, 220, 16, 11, 220, 16, 20, 15, 11, 364, 17, 15, 17, 16, 12, 15, 17, 12, 15, 16, 4567, 320, 18, 11, 220, 17, 11, 220, 16, 23, 15, 11, 364, 17, 15, 17, 16, 12, 15, 16, 12, 15, 16, 4667, 1248, 1920, 15846, 510, 6, 3838, 374, 279, 2790, 8123, 315, 44788, 6088, 553, 1817, 6625, 8987, 11, 10615, 553, 6625, 8987, 20224, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 2077, 25, 364, 4858, 6625, 8987, 842, 11, 829, 11, 30735, 74706, 8, 438, 2790, 26941, 4295, 44788, 47067, 13069, 6625, 8987, 6197, 44788, 47067, 73270, 8987, 842, 284, 6625, 8987, 73270, 8987, 842, 26870, 7710, 6625, 8987, 842, 11, 829, 15520, 7710, 2790, 26941, 16089, 35994, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant specialized in generating SQL queries.\n",
      "Generate an SQL query that correctly answers the user's question based on the provided database schema and context.<|im_end|>\n",
      "<|im_start|>user\n",
      "Context:\n",
      "'CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');'\n",
      "                                    Question:\n",
      "'What is the total volume of timber sold by each salesperson, sorted by salesperson?'<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Result: 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;'<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2077, 25, 364, 4858, 6625, 8987, 842, 11, 829, 11, 30735, 74706, 8, 438, 2790, 26941, 4295, 44788, 47067, 13069, 6625, 8987, 6197, 44788, 47067, 73270, 8987, 842, 284, 6625, 8987, 73270, 8987, 842, 26870, 7710, 6625, 8987, 842, 11, 829, 15520, 7710, 2790, 26941, 16089, 35994, 151645, 198]\n",
      "labels:\n",
      "Result: 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;'<|im_end|>\n",
      "\n",
      "[INFO|configuration_utils.py:693] 2025-05-11 23:11:30,894 >> loading configuration file config.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 23:11:30,896 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[WARNING|2025-05-11 23:11:30] llamafactory.model.model_utils.attention:148 >> FlashAttention-2 is not installed.\n",
      "[INFO|2025-05-11 23:11:30] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "model.safetensors: 100%|███████████████████| 1.50G/1.50G [00:17<00:00, 87.1MB/s]\n",
      "[INFO|modeling_utils.py:1124] 2025-05-11 23:11:50,012 >> loading weights file model.safetensors from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/model.safetensors\n",
      "[INFO|modeling_utils.py:2167] 2025-05-11 23:11:50,160 >> Instantiating Qwen3ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-11 23:11:50,163 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4930] 2025-05-11 23:11:54,525 >> All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4938] 2025-05-11 23:11:54,525 >> All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-0.6B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 239/239 [00:00<00:00, 1.10MB/s]\n",
      "[INFO|configuration_utils.py:1097] 2025-05-11 23:11:54,895 >> loading configuration file generation_config.json from cache at /home/stud2020/0starzyk/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-11 23:11:54,896 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "[INFO|2025-05-11 23:11:54] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-11 23:11:54] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-11 23:11:54] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-11 23:11:54] llamafactory.model.adapter:143 >> Fine-tuning method: DoRA\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[INFO|2025-05-11 23:11:56] llamafactory.model.loader:143 >> trainable params: 2,379,776 || all params: 598,429,696 || trainable%: 0.3977\n",
      "[INFO|trainer.py:748] 2025-05-11 23:11:56,124 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2414] 2025-05-11 23:11:56,528 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-05-11 23:11:56,528 >>   Num examples = 100,000\n",
      "[INFO|trainer.py:2416] 2025-05-11 23:11:56,528 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2417] 2025-05-11 23:11:56,528 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2420] 2025-05-11 23:11:56,528 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2421] 2025-05-11 23:11:56,528 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2422] 2025-05-11 23:11:56,529 >>   Total optimization steps = 9,375\n",
      "[INFO|trainer.py:2423] 2025-05-11 23:11:56,531 >>   Number of trainable parameters = 2,379,776\n",
      "  0%|                                                  | 0/9375 [00:00<?, ?it/s]/home/stud2020/0starzyk/llm-fine-tuning/my-venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train $NOTEBOOK_DIR/LLaMA-Factory/train_qwen3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:43:15.394083700Z",
     "start_time": "2025-05-03T16:43:15.004673300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower().strip().replace(\"\\n\", \" \")\n",
    "    return text\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:45:39.565195200Z",
     "start_time": "2025-05-03T16:43:15.399414900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████| 100000/100000 [01:35<00:00, 1045.73 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████| 5851/5851 [00:05<00:00, 1024.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    input_text = \"Translate to SQL: \" + normalize_text(example[\"sql_prompt\"])\n",
    "    output_text = normalize_text(example[\"sql\"])\n",
    "\n",
    "    model_inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "available_splits = dataset.keys()\n",
    "train_ds = tokenized_ds[\"train\"]\n",
    "validation_ds = tokenized_ds[\"test\"] if \"test\" in available_splits else None\n",
    "test_ds = tokenized_ds[\"test\"] if \"test\" in available_splits else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:47:35.804666900Z",
     "start_time": "2025-05-03T16:45:39.573204300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1210717/3438895915.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/stud2020/0starzyk/llm-fine-tuning/my-venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='62500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   24/62500 02:23 < 112:59:23, 0.15 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./sql_model\",\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=validation_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-03T16:47:35.789336800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(test_dataset):\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_labels = [[label] for label in decoded_labels]\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    results = trainer.evaluate(eval_dataset=test_dataset, compute_metrics=compute_metrics)\n",
    "    return results\n",
    "\n",
    "if test_ds:\n",
    "    test_results = evaluate_model(test_ds)\n",
    "    print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-03T16:47:35.790340500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def driver(question):\n",
    "    inputs = tokenizer(\"Translate to SQL: \" + question, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(inputs, max_length=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(driver(\"Find all customers who ordered in 2023\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
