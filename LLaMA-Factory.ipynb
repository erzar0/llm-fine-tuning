{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df61cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyre/repos/llm-fine-tuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kyre/repos/llm-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os \n",
    "import pandas as pd\n",
    "import sys\n",
    "    \n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "print(NOTEBOOK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f74751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data subsets: dict_keys(['train', 'test'])\n",
      "Features: \n",
      "---> id                            : 5097\n",
      "---> domain                        : forestry\n",
      "---> domain_description            : Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.\n",
      "---> sql_complexity                : single join\n",
      "---> sql_complexity_description    : only one join (specify inner, outer, cross)\n",
      "---> sql_task_type                 : analytics and reporting\n",
      "---> sql_task_type_description     : generating reports, dashboards, and analytical insights\n",
      "---> sql_prompt                    : What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "---> sql_context                   : CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "---> sql                           : SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "---> sql_explanation               : Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/gretelai/synthetic_text_to_sql\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "\n",
    "print(\"Available data subsets:\", dataset.keys())\n",
    "print(\"Features: \")\n",
    "for k, v in dataset[\"train\"][0].items():\n",
    "    print(f\"---> {k:30}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725afaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f49a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved ShareGPT formatted data to 'data/dataset_train.json'\n",
      "Successfully saved ShareGPT formatted data to 'data/dataset_test.json'\n",
      "\n",
      "--- First Record Example ---\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"user\",\n",
      "      \"value\": \"You are provided with the following database schema and context:\\n\\n--- SCHEMA START ---\\nCREATE TABLE creative_ai (application_id INT, name TEXT, region TEXT, explainability_score FLOAT); INSERT INTO creative_ai (application_id, name, region, explainability_score) VALUES (1, 'ApplicationX', 'Europe', 0.87), (2, 'ApplicationY', 'North America', 0.91), (3, 'ApplicationZ', 'Europe', 0.84), (4, 'ApplicationAA', 'North America', 0.93), (5, 'ApplicationAB', 'Europe', 0.89);\\n--- SCHEMA END ---\\n\\nUsing only this information, write an SQL query that answers the following question:\\n\\n\\\"What is the average explainability score of creative AI applications in 'Europe' and 'North America' in the 'creative_ai' table?\\\"\\n\\nOutput only the SQL query, with no additional text.\\nWrap your final query inside <sql> and </sql> tags.\\n/no_think\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"<sql>\\nSELECT AVG(explainability_score) FROM creative_ai WHERE region IN ('Europe', 'North America');\\n</sql>\"\n",
      "    }\n",
      "  ],\n",
      "  \"system\": \"You are a highly precise SQL generation engine.\\nYour sole task is to translate natural language questions into correct, executable SQL queries.\\nYou must only rely on the provided database schema and context.\\nDo not explain or comment.\\nDo not include anything except a valid SQL query enclosed in <sql>...</sql> tags.\\nEnsure the SQL query is syntactically correct and executable.\\nYou are only allowed to use the generic SQL syntax, no specific dialects.\\n/no_think\",\n",
      "  \"tools\": \"\"\n",
      "}\n",
      "mkdir: cannot create directory ‘/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/data/’: File exists\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import create_sharegpt_format\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Convert the dataset to ShareGPT format\n",
    "for df, df_name in [(train_df, \"dataset_train\"), (test_df, \"dataset_test\")]:\n",
    "    sharegpt_data = create_sharegpt_format(df)\n",
    "\n",
    "    output_filename = Path(f'data/{df_name}.json')\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sharegpt_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Successfully saved ShareGPT formatted data to '{output_filename}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON file: {e}\")\n",
    "        \n",
    "if sharegpt_data:\n",
    "    print(\"\\n--- First Record Example ---\")\n",
    "    print(json.dumps(sharegpt_data[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "!mkdir $NOTEBOOK_DIR/LLaMA-Factory/data/\n",
    "!cp -r $NOTEBOOK_DIR/data/ $NOTEBOOK_DIR/LLaMA-Factory/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a445b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1597\n"
     ]
    }
   ],
   "source": [
    "# Check the maximum length of the first conversation string in the ShareGPT data\n",
    "\n",
    "max_len_str = 0\n",
    "for record in sharegpt_data:\n",
    "    max_len_str = max(max_len_str, len(record['conversations'][0]['value']))\n",
    "print(max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9797d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n",
      "/home/kyre/repos/llm-fine-tuning/LLaMA-Factory\n",
      "CITATION.cff    Makefile      \u001b[0m\u001b[01;34mcache\u001b[0m/       \u001b[01;34mexamples\u001b[0m/         setup.py\n",
      "LICENSE         README.md     \u001b[01;34mdata\u001b[0m/        pyproject.toml    \u001b[01;34msrc\u001b[0m/\n",
      "\u001b[01;34mLLaMA-Factory\u001b[0m/  README_zh.md  \u001b[01;34mdocker\u001b[0m/      requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
      "MANIFEST.in     \u001b[01;34massets\u001b[0m/       \u001b[01;34mevaluation\u001b[0m/  \u001b[01;34mscripts\u001b[0m/          train_qwen3.json\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd $NOTEBOOK_DIR/LLaMA-Factory\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae47a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e934aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    # \"deepspeed\": DS_CONFIG_PATH,\n",
    "    \"cutoff_len\": 1024,\n",
    "    \"dataset\": \"train_sql_dataset\",\n",
    "    \"ddp_timeout\": 9000,\n",
    "    \"do_train\": True,\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    \"use_dora\": True,\n",
    "    \"fp16\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"flash_attn\": \"fa2\",\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"logging_steps\": 8,\n",
    "    \"lora_target\": \"q_proj,v_proj\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"output_dir\": \"out\",\n",
    "    \"overwrite_cache\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"plot_loss\": True,\n",
    "    \"report_to\": \"wandb\", # wnadb\n",
    "    \"save_steps\": 250,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"stage\": \"sft\",\n",
    "    \"template\": \"qwen3\",\n",
    "    \"warmup_steps\": 50,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_steps\": 1000\n",
    "}\n",
    "\n",
    "json.dump(training_args, open(f\"{NOTEBOOK_DIR}/LLaMA-Factory/train_qwen3.json\", \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409bb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export $(cat $NOTEBOOK_DIR/.env | xargs)\n",
    "!cd $NOTEBOOK_DIR/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60874811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training command from the terminal\n",
    "# !llamafactory-cli train LLaMA-Factory/train_qwen3.json\n",
    "# !llamafactory-cli export llama-factory-configs/merge_lora.yaml\n",
    "# !llamafactory-cli chat llama-factory-configs/infer_lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d4752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
