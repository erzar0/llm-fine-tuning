{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df61cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyre/repos/llm-fine-tuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-19 23:15:53,480] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kyre/repos/llm-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path(\"/\".join(__vsc_ipynb_file__.split(\"/\")[:-1]))\n",
    "print(NOTEBOOK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f74751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data subsets: dict_keys(['train', 'test'])\n",
      "Features: \n",
      "---> id                            : 5097\n",
      "---> domain                        : forestry\n",
      "---> domain_description            : Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.\n",
      "---> sql_complexity                : single join\n",
      "---> sql_complexity_description    : only one join (specify inner, outer, cross)\n",
      "---> sql_task_type                 : analytics and reporting\n",
      "---> sql_task_type_description     : generating reports, dashboards, and analytical insights\n",
      "---> sql_prompt                    : What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "---> sql_context                   : CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "---> sql                           : SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "---> sql_explanation               : Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/gretelai/synthetic_text_to_sql\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "\n",
    "print(\"Available data subsets:\", dataset.keys())\n",
    "print(\"Features: \")\n",
    "for k, v in dataset[\"train\"][0].items():\n",
    "    print(f\"---> {k:30}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725afaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f49a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved ShareGPT formatted data to 'data/dataset_train.json'\n",
      "\n",
      "--- First Record Example ---\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"user\",\n",
      "      \"value\": \"Context:\\n'CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');'\\n                                    Question:\\n'What is the total volume of timber sold by each salesperson, sorted by salesperson?'\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"Result: 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;'\"\n",
      "    }\n",
      "  ],\n",
      "  \"system\": \"You are a helpful assistant specialized in generating SQL queries.\\nGenerate an SQL query that correctly answers the user's question based on the provided database schema and context.\",\n",
      "  \"tools\": \"\"\n",
      "}\n",
      "mkdir: cannot create directory ‘/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/data/’: File exists\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import create_sharegpt_format\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "sharegpt_data = create_sharegpt_format(train_df)\n",
    "\n",
    "output_filename = Path('data/dataset_train.json')\n",
    "try:\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sharegpt_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Successfully saved ShareGPT formatted data to '{output_filename}'\")\n",
    "\n",
    "    if sharegpt_data:\n",
    "        print(\"\\n--- First Record Example ---\")\n",
    "        print(json.dumps(sharegpt_data[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")\n",
    "\n",
    "!mkdir $NOTEBOOK_DIR/LLaMA-Factory/data/\n",
    "!cp -r $NOTEBOOK_DIR/data/ $NOTEBOOK_DIR/LLaMA-Factory/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a445b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202\n"
     ]
    }
   ],
   "source": [
    "max_len_str = 0\n",
    "for record in sharegpt_data:\n",
    "    max_len_str = max(max_len_str, len(record['conversations'][0]['value']))\n",
    "print(max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9797d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n",
      "/home/kyre/repos/llm-fine-tuning/LLaMA-Factory\n",
      "CITATION.cff    README_zh.md  \u001b[0m\u001b[01;34mexamples\u001b[0m/         \u001b[01;34msrc\u001b[0m/\n",
      "LICENSE         \u001b[01;34massets\u001b[0m/       \u001b[01;34mout\u001b[0m/              \u001b[01;34mtests\u001b[0m/\n",
      "\u001b[01;34mLLaMA-Factory\u001b[0m/  \u001b[01;34mcache\u001b[0m/        pyproject.toml    train_qwen3.json\n",
      "MANIFEST.in     \u001b[01;34mdata\u001b[0m/         requirements.txt\n",
      "Makefile        \u001b[01;34mdocker\u001b[0m/       \u001b[01;34mscripts\u001b[0m/\n",
      "README.md       \u001b[01;34mevaluation\u001b[0m/   setup.py\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd $NOTEBOOK_DIR/LLaMA-Factory\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae47a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e934aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    # \"deepspeed\": DS_CONFIG_PATH,\n",
    "    \"cutoff_len\": 1024,\n",
    "    \"dataset\": \"train_sql_dataset\",\n",
    "    \"ddp_timeout\": 9000,\n",
    "    \"do_train\": True,\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    \"use_dora\": True,\n",
    "    \"fp16\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"flash_attn\": \"fa2\",\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"logging_steps\": 8,\n",
    "    \"lora_target\": \"q_proj,v_proj\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"output_dir\": \"out\",\n",
    "    \"overwrite_cache\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"plot_loss\": True,\n",
    "    \"report_to\": \"wandb\", # wnadb\n",
    "    \"save_steps\": 100,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"stage\": \"sft\",\n",
    "    \"template\": \"qwen3\",\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"max_steps\": 500\n",
    "}\n",
    "\n",
    "json.dump(training_args, open(f\"{NOTEBOOK_DIR}/LLaMA-Factory/train_qwen3.json\", \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409bb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export $(cat $NOTEBOOK_DIR/.env | xargs)\n",
    "!cd $NOTEBOOK_DIR/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60874811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/kyre/repos/llm-fine-tuning/.venv/bin/llamafactory-cli: /home/kyre/repos/llm-fine-tuning/.env/bin/python3: bad interpreter: Not a directory\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train LLaMA-Factory/train_qwen3.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!llamafactory-cli export merge_lora.yaml\n",
    "!llamafactory-cli chat infer_lora.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
